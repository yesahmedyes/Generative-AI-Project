{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia-api in /Users/ahmedharoon/Desktop/Notebooks/env/lib/python3.12/site-packages (0.7.1)\n",
      "Requirement already satisfied: requests in /Users/ahmedharoon/Desktop/Notebooks/env/lib/python3.12/site-packages (from wikipedia-api) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ahmedharoon/Desktop/Notebooks/env/lib/python3.12/site-packages (from requests->wikipedia-api) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ahmedharoon/Desktop/Notebooks/env/lib/python3.12/site-packages (from requests->wikipedia-api) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ahmedharoon/Desktop/Notebooks/env/lib/python3.12/site-packages (from requests->wikipedia-api) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ahmedharoon/Desktop/Notebooks/env/lib/python3.12/site-packages (from requests->wikipedia-api) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install wikipedia-api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 40153 words.\n"
     ]
    }
   ],
   "source": [
    "import wikipediaapi\n",
    "import re\n",
    "\n",
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "    \"ScienceDatasetScript/1.0 (ahmed.dys99@gmail.com)\",\n",
    "    \"en\"\n",
    ")\n",
    "\n",
    "topics = [\n",
    "    \"2024\",\n",
    "    \"2023\",\n",
    "    \"2022\",\n",
    "    \"2021\",\n",
    "    \"2020\",\n",
    "    \"2019\",\n",
    "]\n",
    "\n",
    "word_count_target = 200_000\n",
    "current_word_count = 0\n",
    "compiled_text = \"\"\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"\\[.*?\\]+\", \"\", text)\n",
    "    text = re.sub(r\"\\n+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "for topic in topics:\n",
    "    page = wiki_wiki.page(topic)\n",
    "    if page.exists():\n",
    "        text = clean_text(page.text)\n",
    "\n",
    "        words_in_text = len(text.split())\n",
    "\n",
    "        if current_word_count + words_in_text > word_count_target:\n",
    "            remaining_words = word_count_target - current_word_count\n",
    "            compiled_text += \" \".join(text.split()[:remaining_words])\n",
    "            current_word_count += remaining_words\n",
    "            break\n",
    "        else:\n",
    "            compiled_text += text + \"\\n\\n\"\n",
    "            current_word_count += words_in_text\n",
    "    else:\n",
    "        print(f\"Page for '{topic}' does not exist.\")\n",
    "\n",
    "print(f\"Collected {current_word_count} words.\")\n",
    "\n",
    "with open(\"current_events_dataset.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(compiled_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 24657 words.\n"
     ]
    }
   ],
   "source": [
    "import wikipediaapi\n",
    "import re\n",
    "\n",
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "    \"GeneralDatasetScript/1.0 (ahmed.dys99@gmail.com)\",\n",
    "    \"en\"\n",
    ")\n",
    "\n",
    "topics = [\n",
    "    \"2018\",\n",
    "    \"2017\",\n",
    "    \"2016\",\n",
    "    \"2015\",\n",
    "    \"2014\",\n",
    "    \"2013\",\n",
    "    \"2012\",\n",
    "    \"2011\",\n",
    "    \"2010\",\n",
    "]\n",
    "\n",
    "word_count_target = 200_000\n",
    "current_word_count = 0\n",
    "compiled_text = \"\"\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"\\[.*?\\]+\", \"\", text)\n",
    "    text = re.sub(r\"\\n+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "for topic in topics:\n",
    "    page = wiki_wiki.page(topic)\n",
    "    if page.exists():\n",
    "        text = clean_text(page.text)\n",
    "\n",
    "        words_in_text = len(text.split())\n",
    "\n",
    "        if current_word_count + words_in_text > word_count_target:\n",
    "            remaining_words = word_count_target - current_word_count\n",
    "            compiled_text += \" \".join(text.split()[:remaining_words])\n",
    "            current_word_count += remaining_words\n",
    "            break\n",
    "        else:\n",
    "            compiled_text += text + \"\\n\\n\"\n",
    "            current_word_count += words_in_text\n",
    "    else:\n",
    "        print(f\"Page for '{topic}' does not exist.\")\n",
    "\n",
    "print(f\"Collected {current_word_count} words.\")\n",
    "\n",
    "with open(\"general_dataset.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(compiled_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
