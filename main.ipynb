{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecAdam(torch.optim.Optimizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr=1e-4,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-8,\n",
    "        weight_decay=0,\n",
    "        rectification=True,\n",
    "        pretrain_step=0,\n",
    "        total_step=1000,\n",
    "        k=0.5,\n",
    "        init_beta=10.0,\n",
    "        final_beta=0.1,\n",
    "    ):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "\n",
    "        super(RecAdam, self).__init__(params, defaults)\n",
    "\n",
    "        self.rectification = rectification\n",
    "        self.pretrain_step = pretrain_step\n",
    "        self.total_step = total_step\n",
    "        self.k = k\n",
    "        self.init_beta = init_beta\n",
    "        self.final_beta = final_beta\n",
    "\n",
    "        self.beta = init_beta\n",
    "        self.current_step = 0\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        if self.rectification:\n",
    "            self.beta = self.init_beta - (self.init_beta - self.final_beta) * min(\n",
    "                1, self.current_step / self.total_step\n",
    "            )\n",
    "            self.current_step += 1\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                grad = p.grad.data\n",
    "\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError(\"RecAdam does not support sparse gradients\")\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state[\"step\"] = 0\n",
    "                    state[\"exp_avg\"] = torch.zeros_like(p.data)\n",
    "                    state[\"exp_avg_sq\"] = torch.zeros_like(p.data)\n",
    "\n",
    "                    if hasattr(p, \"pre_trained_params\"):\n",
    "                        state[\"theta\"] = p.pre_trained_params\n",
    "                    else:\n",
    "                        state[\"theta\"] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state[\"exp_avg\"], state[\"exp_avg_sq\"]\n",
    "                beta1, beta2 = group[\"betas\"]\n",
    "\n",
    "                state[\"step\"] += 1\n",
    "\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "\n",
    "                denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state[\"step\"]\n",
    "                bias_correction2 = 1 - beta2 ** state[\"step\"]\n",
    "                step_size = group[\"lr\"] * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                if self.rectification and hasattr(p, \"pre_trained_params\"):\n",
    "                    p.data.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "                    p.data.add_(\n",
    "                        (p.data - state[\"theta\"]), alpha=-self.beta * group[\"lr\"]\n",
    "                    )\n",
    "                else:\n",
    "                    p.data.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "\n",
    "                if group[\"weight_decay\"] != 0:\n",
    "                    p.data.add_(p.data, alpha=-group[\"weight_decay\"] * group[\"lr\"])\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 12\n",
    "    n_heads: int = 12\n",
    "    n_embd: int = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank=64, alpha=128):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.lora_A = nn.Parameter(torch.zeros(in_features, rank))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(rank, out_features))\n",
    "        self.scaling = alpha / rank\n",
    "\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (x @ (self.lora_A @ self.lora_B)) * self.scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KAdapter(nn.Module):\n",
    "    def __init__(self, config: GPTConfig, k=2):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.adapters = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(config.n_embd, config.n_embd // 2),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(config.n_embd // 2, config.n_embd),\n",
    "                )\n",
    "                for _ in range(k)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        for adapter in self.adapters:\n",
    "            nn.init.normal_(adapter[0].weight, std=0.02)\n",
    "            nn.init.zeros_(adapter[0].bias)\n",
    "            nn.init.normal_(adapter[2].weight, std=0.02)\n",
    "            nn.init.zeros_(adapter[2].bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = 0\n",
    "\n",
    "        for adapter in self.adapters:\n",
    "            output += adapter(x)\n",
    "\n",
    "        return output / self.k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config: GPTConfig, lora: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        assert config.n_embd % config.n_heads == 0\n",
    "\n",
    "        self.n_heads = config.n_heads\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "\n",
    "        \"\"\"\n",
    "        equivalent to:\n",
    "        \n",
    "        for i in range(config.n_heads): \n",
    "            self.key = nn.Linear(config.n_embd, config.head_size)\n",
    "            self.query = nn.Linear(config.n_embd, config.head_size)\n",
    "            self.value = nn.Linear(config.n_embd, config.head_size)\n",
    "        \"\"\"\n",
    "\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "\n",
    "        self.c_proj.SCALE_INIT = 1\n",
    "\n",
    "        self.lora = lora\n",
    "\n",
    "        if lora:\n",
    "            self.lora_attn = LoRALayer(config.n_embd, 3 * config.n_embd)\n",
    "            self.lora_proj = LoRALayer(config.n_embd, config.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = (\n",
    "            x.size()\n",
    "        )  # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        qkv = self.c_attn(x)\n",
    "\n",
    "        if self.lora:\n",
    "            qkv = qkv + self.lora_attn(x)\n",
    "\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "\n",
    "        k = k.view(B, T, self.n_heads, C // self.n_heads).transpose(\n",
    "            1, 2\n",
    "        )  # B, n_heads, T, head_size\n",
    "        q = q.view(B, T, self.n_heads, C // self.n_heads).transpose(\n",
    "            1, 2\n",
    "        )  # B, n_heads, T, head_size\n",
    "        v = v.view(B, T, self.n_heads, C // self.n_heads).transpose(\n",
    "            1, 2\n",
    "        )  # B, n_heads, T, head_size\n",
    "\n",
    "        # attn = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) # B, n_heads, T, T\n",
    "        # attn = attn.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
    "        # attn = F.softmax(attn, dim=-1)\n",
    "        # y = attn @ v # B, n_heads, T, head_size\n",
    "\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)  # B, T, C\n",
    "\n",
    "        out = self.c_proj(y)\n",
    "\n",
    "        if self.lora:\n",
    "            out = out + self.lora_proj(y)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: GPTConfig, lora: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = nn.GELU(approximate=\"tanh\")\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "\n",
    "        self.c_proj.SCALE_INIT = 1\n",
    "\n",
    "        self.lora = lora\n",
    "\n",
    "        if lora:\n",
    "            self.lora_fc = LoRALayer(config.n_embd, 4 * config.n_embd)\n",
    "            self.lora_proj = LoRALayer(4 * config.n_embd, config.n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        fc_out = self.c_fc(x)\n",
    "\n",
    "        if self.lora:\n",
    "            fc_out = fc_out + self.lora_fc(x)\n",
    "\n",
    "        fc_out = self.gelu(fc_out)\n",
    "\n",
    "        proj_out = self.c_proj(fc_out)\n",
    "\n",
    "        if self.lora:\n",
    "            proj_out = proj_out + self.lora_proj(fc_out)\n",
    "\n",
    "        return proj_out\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: GPTConfig,\n",
    "        lora: bool = False,\n",
    "        k_adapters: int = 0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config, lora)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config, lora)\n",
    "\n",
    "        self.k_adapters = k_adapters\n",
    "\n",
    "        if k_adapters > 0:\n",
    "            self.adapters = KAdapter(config, k_adapters)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "\n",
    "        if self.k_adapters > 0:\n",
    "            x = x + self.adapters(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: GPTConfig,\n",
    "        lora: bool = False,\n",
    "        k_adapters: int = 0,\n",
    "        use_modular: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        if use_modular:\n",
    "            self.modular = nn.ModuleDict(\n",
    "                dict(\n",
    "                    wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
    "                    wpe=nn.Embedding(config.block_size, config.n_embd),\n",
    "                    h=nn.ModuleList(\n",
    "                        [Block(config, lora, k_adapters) for _ in range(config.n_layer)]\n",
    "                    ),\n",
    "                    ln_f=nn.LayerNorm(config.n_embd),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            self.modular.apply(self._init_weights)\n",
    "\n",
    "            self.projection = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
    "            self.scale_factor = 0.05\n",
    "\n",
    "        self.transformer = nn.ModuleDict(\n",
    "            dict(\n",
    "                wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
    "                wpe=nn.Embedding(config.block_size, config.n_embd),\n",
    "                h=nn.ModuleList(\n",
    "                    [Block(config, lora, k_adapters) for _ in range(config.n_layer)]\n",
    "                ),\n",
    "                ln_f=nn.LayerNorm(config.n_embd),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        self.use_modular = use_modular\n",
    "\n",
    "        self.weight_masks = {}\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "\n",
    "            if hasattr(module, \"SCALE_INIT\"):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def set_weight_masks(self, masks):\n",
    "        for name, mask in masks.items():\n",
    "            if name in dict(self.named_parameters()):\n",
    "                param = dict(self.named_parameters())[name]\n",
    "\n",
    "                mask = mask.to(device)\n",
    "\n",
    "                def grad_hook(grad, mask=mask):\n",
    "                    return grad * mask\n",
    "\n",
    "                param.register_hook(grad_hook)\n",
    "\n",
    "                self.weight_masks[name] = mask\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.size()\n",
    "\n",
    "        assert T <= self.config.block_size\n",
    "\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if self.use_modular:\n",
    "            tok_emb_mod = self.modular.wte(idx)\n",
    "            pos_emb_mod = self.modular.wpe(pos)\n",
    "            x_mod = tok_emb_mod + pos_emb_mod\n",
    "\n",
    "            for block in self.modular.h:\n",
    "                x_mod = block(x_mod)\n",
    "\n",
    "            x_mod = self.modular.ln_f(x_mod)\n",
    "\n",
    "            x = x + (self.scale_factor * self.projection(x_mod))\n",
    "\n",
    "        logits = self.lm_head(x)  # B, T, vocab_size\n",
    "\n",
    "        loss = None\n",
    "\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(\n",
    "        cls,\n",
    "        override_args=None,\n",
    "        lora: bool = False,\n",
    "        k_adapters: int = 0,\n",
    "        use_modular: bool = False,\n",
    "    ):\n",
    "        override_args = override_args or {}\n",
    "\n",
    "        assert all(k == \"dropout\" for k in override_args)\n",
    "\n",
    "        from transformers import GPT2LMHeadModel\n",
    "\n",
    "        config = GPTConfig()\n",
    "        model = GPT(config, lora=lora, k_adapters=k_adapters, use_modular=use_modular)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith(\".attn.bias\")]\n",
    "\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith(\".attn.masked_bias\")]\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith(\".attn.bias\")]\n",
    "\n",
    "        transposed = [\n",
    "            \"attn.c_attn.weight\",\n",
    "            \"attn.c_proj.weight\",\n",
    "            \"mlp.c_fc.weight\",\n",
    "            \"mlp.c_proj.weight\",\n",
    "        ]\n",
    "\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        if use_modular:\n",
    "            with torch.no_grad():\n",
    "                for k, v in sd.items():\n",
    "                    if k.startswith(\"transformer.\"):\n",
    "                        modular_key = k.replace(\"transformer.\", \"modular.\")\n",
    "                        if modular_key in sd:\n",
    "                            sd[modular_key].copy_(v)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate):\n",
    "        base_params = []\n",
    "        adapt_params = []\n",
    "\n",
    "        for name, param in self.named_parameters():\n",
    "            if any(x in name for x in [\"lora_A\", \"lora_B\", \"adapters\", \"modular\"]):\n",
    "                adapt_params.append(param)\n",
    "            else:\n",
    "                base_params.append(param)\n",
    "\n",
    "        use_fused = \"cuda\" in device\n",
    "\n",
    "        recadam_optimizer = RecAdam(\n",
    "            [{\"params\": base_params}],\n",
    "            lr=learning_rate * 0.1,\n",
    "            betas=(0.9, 0.95),\n",
    "            eps=1e-8,\n",
    "            weight_decay=weight_decay,\n",
    "            rectification=True,\n",
    "            pretrain_step=0,\n",
    "            total_step=200,\n",
    "            k=0.5,\n",
    "            init_beta=10.0,\n",
    "            final_beta=0.1,\n",
    "        )\n",
    "\n",
    "        adamw_optimizer = torch.optim.AdamW(\n",
    "            [{\"params\": adapt_params}],\n",
    "            lr=learning_rate,\n",
    "            betas=(0.9, 0.95),\n",
    "            eps=1e-8,\n",
    "            weight_decay=weight_decay,\n",
    "            fused=use_fused,\n",
    "        )\n",
    "\n",
    "        return [recadam_optimizer, adamw_optimizer]\n",
    "\n",
    "    def hyper_scale(self):\n",
    "        original_wte = self.transformer.wte.weight.data\n",
    "        original_wpe = self.transformer.wpe.weight.data\n",
    "\n",
    "        self.transformer.wte = nn.Embedding(\n",
    "            self.config.vocab_size, 2 * self.config.n_embd\n",
    "        )\n",
    "        self.transformer.wpe = nn.Embedding(\n",
    "            self.config.block_size, 2 * self.config.n_embd\n",
    "        )\n",
    "\n",
    "        self.transformer.wte.weight.data = torch.cat([original_wte] * 2, dim=1)\n",
    "        self.transformer.wpe.weight.data = torch.cat([original_wpe] * 2, dim=1)\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            # Layer Norm 1\n",
    "\n",
    "            original_ln_1_weight = block.ln_1.weight.data\n",
    "            original_ln_1_bias = block.ln_1.bias.data\n",
    "\n",
    "            block.ln_1 = nn.LayerNorm(2 * self.config.n_embd)\n",
    "\n",
    "            block.ln_1.weight.data = torch.cat([original_ln_1_weight] * 2, dim=0)\n",
    "            block.ln_1.bias.data = torch.cat([original_ln_1_bias] * 2, dim=0)\n",
    "\n",
    "            block.attn.n_heads = 2 * block.attn.n_heads\n",
    "            block.attn.n_embd = 2 * block.attn.n_embd\n",
    "\n",
    "            # Attention Head\n",
    "\n",
    "            original_c_attn_weight = block.attn.c_attn.weight.data\n",
    "            original_c_attn_bias = block.attn.c_attn.bias.data\n",
    "\n",
    "            q_w, k_w, v_w = original_c_attn_weight.chunk(3, dim=0)\n",
    "            q_b, k_b, v_b = original_c_attn_bias.chunk(3)\n",
    "\n",
    "            block.attn.c_attn = nn.Linear(\n",
    "                2 * self.config.n_embd, 6 * self.config.n_embd\n",
    "            )\n",
    "\n",
    "            new_weight = (\n",
    "                torch.cat(\n",
    "                    [q_w, q_w, k_w, k_w, v_w, v_w],\n",
    "                    dim=0,\n",
    "                )\n",
    "            ) / 2\n",
    "            new_weight = torch.cat([new_weight] * 2, dim=1)\n",
    "\n",
    "            new_bias = torch.cat([q_b, q_b, k_b, k_b, v_b, v_b])\n",
    "\n",
    "            block.attn.c_attn.weight.data = new_weight\n",
    "            block.attn.c_attn.bias.data = new_bias\n",
    "\n",
    "            original_c_proj_weight = block.attn.c_proj.weight.data\n",
    "            original_c_proj_bias = block.attn.c_proj.bias.data\n",
    "\n",
    "            block.attn.c_proj = nn.Linear(\n",
    "                2 * self.config.n_embd, 2 * self.config.n_embd\n",
    "            )\n",
    "\n",
    "            block.attn.c_proj.weight.data = (\n",
    "                torch.cat(\n",
    "                    [torch.cat([original_c_proj_weight] * 2, dim=1)] * 2,\n",
    "                    dim=0,\n",
    "                )\n",
    "                / 2\n",
    "            )\n",
    "\n",
    "            block.attn.c_proj.bias.data = torch.cat([original_c_proj_bias] * 2, dim=0)\n",
    "\n",
    "            # MLP\n",
    "\n",
    "            original_c_fc_weight = block.mlp.c_fc.weight.data\n",
    "            original_c_fc_bias = block.mlp.c_fc.bias.data\n",
    "\n",
    "            block.mlp.c_fc = nn.Linear(2 * self.config.n_embd, 8 * self.config.n_embd)\n",
    "\n",
    "            block.mlp.c_fc.weight.data = (\n",
    "                torch.cat(\n",
    "                    [torch.cat([original_c_fc_weight] * 2, dim=1)] * 2,\n",
    "                    dim=0,\n",
    "                )\n",
    "                / 2\n",
    "            )\n",
    "\n",
    "            block.mlp.c_fc.bias.data = torch.cat([original_c_fc_bias] * 2, dim=0)\n",
    "\n",
    "            original_c_proj_weight = block.mlp.c_proj.weight.data\n",
    "            original_c_proj_bias = block.mlp.c_proj.bias.data\n",
    "\n",
    "            block.mlp.c_proj = nn.Linear(8 * self.config.n_embd, 2 * self.config.n_embd)\n",
    "\n",
    "            block.mlp.c_proj.weight.data = (\n",
    "                torch.cat(\n",
    "                    [torch.cat([original_c_proj_weight] * 2, dim=1)] * 2,\n",
    "                    dim=0,\n",
    "                )\n",
    "                / 2\n",
    "            )\n",
    "\n",
    "            block.mlp.c_proj.bias.data = torch.cat([original_c_proj_bias] * 2, dim=0)\n",
    "\n",
    "            # Layer Norm 2\n",
    "\n",
    "            original_ln_2_weight = block.ln_2.weight.data\n",
    "            original_ln_2_bias = block.ln_2.bias.data\n",
    "\n",
    "            block.ln_2 = nn.LayerNorm(2 * self.config.n_embd)\n",
    "\n",
    "            block.ln_2.weight.data = torch.cat([original_ln_2_weight] * 2, dim=0)\n",
    "            block.ln_2.bias.data = torch.cat([original_ln_2_bias] * 2, dim=0)\n",
    "\n",
    "        original_ln_f_weight = self.transformer.ln_f.weight.data\n",
    "        original_ln_f_bias = self.transformer.ln_f.bias.data\n",
    "\n",
    "        self.transformer.ln_f = nn.LayerNorm(2 * self.config.n_embd)\n",
    "\n",
    "        self.transformer.ln_f.weight.data = torch.cat([original_ln_f_weight] * 2, dim=0)\n",
    "        self.transformer.ln_f.bias.data = torch.cat([original_ln_f_bias] * 2, dim=0)\n",
    "\n",
    "        original_lm_head = self.lm_head.weight.data\n",
    "\n",
    "        self.lm_head = nn.Linear(\n",
    "            2 * self.config.n_embd, self.config.vocab_size, bias=False\n",
    "        )\n",
    "\n",
    "        self.lm_head.weight.data = torch.cat([original_lm_head] * 2, dim=1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT.from_pretrained(lora=True, k_adapters=0, use_modular=False)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "model = torch.compile(model, backend=\"aot_eager\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.hyper_scale()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 133,876,992\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "\n",
    "\n",
    "total_params, trainable_params = count_parameters(model)\n",
    "\n",
    "print(f\"Total Parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 16384, Gradient accumulation steps: 4\n"
     ]
    }
   ],
   "source": [
    "total_batch_size = 16384\n",
    "B = 4\n",
    "T = model.config.block_size\n",
    "\n",
    "assert total_batch_size % (B * T) == 0\n",
    "\n",
    "grad_accumulation_steps = total_batch_size // (B * T)\n",
    "\n",
    "print(\n",
    "    f\"Batch size: {total_batch_size}, Gradient accumulation steps: {grad_accumulation_steps}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, B, T, fileName):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        with open(fileName, \"r\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        encoder = tiktoken.get_encoding(\"gpt2\")\n",
    "        tokens = encoder.encode(text)\n",
    "\n",
    "        self.tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "        self.curr = self.B * self.T\n",
    "\n",
    "    def next(self):\n",
    "        B, T = self.B, self.T\n",
    "\n",
    "        buffer = self.tokens[self.curr : self.curr + B * T + 1]\n",
    "\n",
    "        x = buffer[:-1].view(B, T)\n",
    "        y = buffer[1:].view(B, T)\n",
    "\n",
    "        self.curr += B * T  \n",
    "\n",
    "        if self.curr + (B * T + 1) > len(self.tokens):\n",
    "            self.curr = B * T\n",
    "\n",
    "        return x, y\n",
    "    \n",
    "    def reset(self):\n",
    "        self.curr = 0\n",
    "\n",
    "\n",
    "data_loader = DataLoader(B=B, T=T, fileName=\"data/current_events_dataset.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = model.configure_optimizers(weight_decay=0.1, learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval2(fileName):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_batches = 0\n",
    "    \n",
    "    eval_data = torch.load(fileName, weights_only=True, map_location=device)\n",
    "\n",
    "    x_eval = eval_data['context']\n",
    "    y_eval = eval_data['target']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_batches = x_eval.shape[0] // B\n",
    "        \n",
    "        for i in range(0, total_batches):\n",
    "            x, y = x_eval[i : i + B], y_eval[i : i + B]    \n",
    "\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            logits, _ = model(x)\n",
    "            \n",
    "            logits = logits[:, -1, :]\n",
    "            \n",
    "            loss = F.cross_entropy(logits, y)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / total_batches\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if not any(x in name for x in [\"lora_A\", \"lora_B\", \"adapters\", \"modular\"]):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "while i <= 10:\n",
    "    for opt in optimizers:\n",
    "        opt.zero_grad()\n",
    "\n",
    "    loss_acum = 0.0\n",
    "\n",
    "    for micro_step in range(grad_accumulation_steps):\n",
    "        x, y = data_loader.next()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        logits, loss = model(x, y)\n",
    "\n",
    "        loss = loss / grad_accumulation_steps\n",
    "\n",
    "        loss_acum += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    for opt in optimizers:\n",
    "        opt.step()\n",
    "    \n",
    "    general_loss = eval2(\"data/general_eval.pt\")\n",
    "    current_events_loss = eval2(\"data/current_events_eval.pt\")\n",
    "    \n",
    "    print(f\"Epoch {i}, General Loss: {general_loss}, Current Events Loss: {current_events_loss}\")\n",
    "        \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General Loss 4.279356412887573\n",
      "Current Events Loss 6.460156536102295\n",
      "Ratio 0.6624230216361758\n"
     ]
    }
   ],
   "source": [
    "general_loss = eval2(\"data/general_eval.pt\")\n",
    "current_events_loss = eval2(\"data/current_events_eval.pt\")\n",
    "\n",
    "print(\"General Loss\", general_loss)\n",
    "print(\"Current Events Loss\", current_events_loss)\n",
    "print(\"Ratio\", general_loss / current_events_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
